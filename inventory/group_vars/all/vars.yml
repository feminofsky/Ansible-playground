# inventory/group_vars/all/vars.yml
# Global variables shared across all nodes

# ─── SSH & Users ────────────────────────────────────
deploy_user: deploy
deploy_password: "{{ vault_deploy_password }}"   # Store in vault.yml
ssh_port: 2222
ssh_public_key: "{{ lookup('file', '~/.ssh/id_ed25519.pub') }}"

# ─── Domain & Email ─────────────────────────────────
domain: blumefy.com
admin_email: techops@blumefy.com

# ─── Node IPs (for /etc/hosts and firewall rules) ───
node1_ip: "161.97.69.48"
node2_ip: "161.97.85.133"

# ─── WireGuard VPN ─────────────────────────────────
# Cluster + etcd + API over private VPN; no public exposure
wireguard_enabled: true
wireguard_subnet: "10.10.10.0/24"
wireguard_node1_ip: "10.10.10.1"
wireguard_node2_ip: "10.10.10.2"
wireguard_deploy_ip: "10.10.10.10"
# wireguard_full_vpn: true = UFW allows only VPN (10.10.10.0/24). Use for fresh install or after migrate-to-vpn.
# wireguard_full_vpn: false = UFW allows VPN + node public IPs (for migration or debugging).
wireguard_full_vpn: true
wireguard_allowed_sources: "{{ [wireguard_subnet] if (wireguard_full_vpn | default(false)) else ([wireguard_subnet] + [node1_ip, node2_ip]) if (wireguard_enabled | default(false)) else [node1_ip, node2_ip] }}"
k3s_api_allowed_sources: "{{ [wireguard_subnet] if (wireguard_enabled | default(false)) else (k3s_api_allowed_ips | default([])) }}"
# Public access: ONLY 2222 (SSH), 80, 443 (Traefik). WireGuard 51820 optional for VPN.
wireguard_allow_public: true   # false = max lockout; VPN needs SSH tunnel or trusted network

# ─── K3s ────────────────────────────────────────────
k3s_version: "v1.34.0+k3s1"   # Kubernetes 1.34+; latest: v1.35.1+k3s1
k3s_token: "{{ vault_k3s_token }}"        # Store in vault.yml
k3s_api_port: 6443
# When wireguard_enabled: API only from VPN. Else: k3s_api_allowed_ips (CIDRs)
# WireGuard: set automatically. Non-VPN: ["1.2.3.4/32"] for admin IPs
k3s_api_allowed_ips: []

# ─── Team member Kubernetes access (kubectl only, no SSH) ─
# Run: ansible-playbook playbooks/site.yml --tags k8s_access
# Kubeconfigs saved to kubeconfigs/<name>-kubeconfig.yml
# Add their IP to k3s_api_allowed_ips so they can reach the API (port 6443 only)
# role: cluster-admin (full) | edit (read+write) | view (read-only)
k8s_team_members:
  - name: alice
    role: cluster-admin
k3s_config_dir: /etc/rancher/k3s
k3s_data_dir: /var/lib/rancher/k3s

# ─── MetalLB ────────────────────────────────────────
metallb_version: "v0.15.3"
# LoadBalancer IPs: use node1 public IP so Traefik ingress is reachable from internet
metallb_ip_pool: "{{ node1_ip }}/32"

# ─── Traefik ────────────────────────────────────────
traefik_version: "39.0.1"   # Helm chart version (Traefik v3)
traefik_namespace: traefik
traefik_helm_wait_timeout: "10m"   # Helm --wait timeout; increase if upgrade times out (e.g. slow image pull)
traefik_crd_api_group: traefik.io   # v3 uses traefik.io; v2 used traefik.containo.us
traefik_replicas: 1          # 1 replica: PVC is ReadWriteOnce, can't spread across nodes
traefik_dashboard_exposed: false   # Set true when vpn_internal_ui_enabled
vpn_internal_ui_enabled: true      # Expose internal UIs at *.blumefy.local (VPN-only, self-signed TLS)
vpn_internal_domain: blumefy.local # .local avoids confusion with public blumefy.com
traefik_vpn_node: node1            # K8s node with WireGuard (10.10.10.1). Override if your node has different name.
vpn_use_dnsmasq: true   # false = use add-vpn-hosts.sh for *.blumefy.local; avoids breaking K8s/Lens/general DNS on Mac

# Cloudflare proxy in front of Traefik (orange cloud). When true: Traefik trusts CF IPs for real client IP; UFW allows 80/443 only from Cloudflare + VPN.
traefik_cloudflare_proxy_enabled: true
# Optional: require Cloudflare client cert on 443 (Authenticated Origin Pulls). Needs roles/traefik/files/cloudflare-origin-pull-ca.pem.
# traefik_cloudflare_origin_pull_enabled: false
# Cloudflare IP ranges (used by Traefik forwardedHeaders and by UFW when traefik_cloudflare_proxy_enabled). Update from https://www.cloudflare.com/ips/
traefik_cloudflare_ip_ranges:
  - 173.245.48.0/20
  - 103.21.244.0/22
  - 103.22.200.0/22
  - 103.31.4.0/22
  - 141.101.64.0/18
  - 108.162.192.0/18
  - 190.93.240.0/20
  - 188.114.96.0/20
  - 197.234.240.0/22
  - 198.41.128.0/17
  - 162.158.0.0/15
  - 104.16.0.0/13
  - 104.24.0.0/14
  - 172.64.0.0/13
  - 131.0.72.0/22
  - 2400:cb00::/32
  - 2606:4700::/32
  - 2803:f800::/32
  - 2405:b500::/32
  - 2405:8100::/32
  - 2a06:98c0::/29
  - 2c0f:f248::/32

# ─── Infisical (secrets management) ───────────────────
infisical_enabled: true
infisical_namespace: infisical
infisical_version: "v0.158.2"
infisical_replicas: 1
infisical_postgres_size: 8Gi
infisical_redis_size: 8Gi
# DB name set by Infisical standalone Helm chart (often infisicalDB)
infisical_postgres_database: infisicalDB
# SITE_URL for redirects when using port-forward; not exposed via Traefik
infisical_site_url: "{{ 'https://infisical.' + (vpn_internal_domain | default('blumefy.local')) if (vpn_internal_ui_enabled | default(false)) else 'http://localhost' }}"
# Pin entire stack to node1: avoids cross-node DNS (EAI_AGAIN) over VPN/Flannel in K3s
infisical_node: node1

# ─── Monitoring ─────────────────────────────────────
monitoring_enabled: false   # Set to true to install Prometheus + Grafana
monitoring_namespace: monitoring
grafana_admin_password: "{{ vault_grafana_password }}"   # Store in vault.yml

# ─── Fail2Ban ───────────────────────────────────────
fail2ban_maxretry: 3
fail2ban_bantime: 86400
fail2ban_findtime: 600

# ─── Argo CD (GitOps) ──────────────────────────────
# AppProjects: dev, prod (each allows deploying to its namespace)
# ApplicationSet watches applications/ in the Git repo and creates Applications automatically.
# For private repos: vault_argocd_repo_ssh_key in vault.yml OR argocd_repo_ssh_key_file (path to key)
argocd_repo_ssh_key_file: "{{ playbook_dir }}/../argocd-deploy"   # key in project root
argocd_enabled: true
argocd_projects: [dev, prod]
# Git repo URL for ApplicationSet (watches applications/*.yaml in this repo)
argocd_gitops_repo_url: "git@github.com:Blumefy/gitops-services.git"   # e.g. https://github.com/org/gitops-services.git or git@github.com:org/gitops-services.git
argocd_gitops_repo_revision: main
# Legacy: manual app list (when argocd_gitops_repo_url not set)
argocd_applications: []

# ─── Namespaces & Dev/Prod Stack (Phase 7) ──────────
prod_namespace: prod
dev_namespace: dev
dev_redis_version: "25.3.0"
dev_rabbitmq_version: "15.0.7"   # 15.0.9 doesn't exist; 15.0.7 is latest 15.0.x
dev_redis_storage: 1Gi
dev_rabbitmq_storage: 1Gi
prod_redis_version: "25.3.0"
prod_rabbitmq_version: "15.0.7"
prod_redis_storage: 2Gi
prod_rabbitmq_storage: 2Gi

namespace_stacks:
  - name: dev
    namespace: "{{ dev_namespace }}"
    redis_password: "{{ vault_dev_redis_password }}"
    redis_storage: "{{ dev_redis_storage }}"
    redis_version: "{{ dev_redis_version }}"
    rabbitmq_user: "{{ vault_dev_rabbitmq_user }}"
    rabbitmq_password: "{{ vault_dev_rabbitmq_password }}"
    rabbitmq_storage: "{{ dev_rabbitmq_storage }}"
    rabbitmq_version: "{{ dev_rabbitmq_version }}"
  - name: prod
    namespace: "{{ prod_namespace }}"
    redis_password: "{{ vault_prod_redis_password }}"
    redis_storage: "{{ prod_redis_storage }}"
    redis_version: "{{ prod_redis_version }}"
    rabbitmq_user: "{{ vault_prod_rabbitmq_user }}"
    rabbitmq_password: "{{ vault_prod_rabbitmq_password }}"
    rabbitmq_storage: "{{ prod_rabbitmq_storage }}"
    rabbitmq_version: "{{ prod_rabbitmq_version }}"

# ─── VPN-only internal UIs (*.blumefy.local) ─
# Exposed via Traefik with IP allowlist to wireguard_subnet only. Self-signed TLS.
# Run add-vpn-hosts.sh in wireguard-configs/ when connecting.
vpn_internal_hosts:
  - dashboard.blumefy.local   # Internal apps landing page
  - traefik.blumefy.local    # Traefik dashboard
  - argocd.blumefy.local     # Argo CD UI
  - infisical.blumefy.local  # Infisical UI
  - rabbitmq-dev.blumefy.local  # RabbitMQ Management — dev
  - rabbitmq.blumefy.local   # RabbitMQ Management — prod
  - redis-dev.blumefy.local  # Redis Commander UI — dev
  - redis.blumefy.local      # Redis Commander UI — prod
  - metallb.blumefy.local    # MetalLB has no UI; host resolves for convenience

# ─── Subdomains ─────────────────────────────────────
# IngressRoutes for prod/dev apps. Set when you have matching K8s Services deployed.
# Example: subdomains: [{name: admin, service: admin-service, port: 80, namespace: prod}, ...]
subdomains: []

# ─── Rolling update order (playbooks/update.yml) ─────
# Update non-init nodes first, then init node last. Derived from inventory.
update_init_node: "{{ groups['control_plane'] | first }}"
update_other_nodes: "{{ groups['control_plane'] | difference([update_init_node]) | list }}"
